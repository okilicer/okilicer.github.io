<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Projects - Dr. Orsan Kilicer</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="wrapper">
        <header>
            <h1>Machine Learning Projects</h1>
            <p>Research Highlights and Code Examples</p>
        </header>

        <nav aria-label="Main navigation">
            <a href="index.html">Home</a>
            <a href="about.html">About</a>
            <a href="machine_learning.html" class="active">Machine Learning</a>
            <a href="publications.html">Publications</a>
            <a href="contact.html">Contact</a>
        </nav>

        <main>
            <section id="mnist-project">
                <h2>MNIST TensorFlow Project</h2>

                <h3>Project Description</h3>
                <p>
                    This project uses TensorFlow to classify handwritten digits from the MNIST dataset.
                    It includes data loading, preprocessing, building a neural network, training, and evaluating accuracy.
                    You can explore both the full PDF explanation and the Jupyter Notebook.
                </p>

                <h3>Project Files</h3>
                <ul>
                    <li>üìÑ <a href="mnist_tensorflow_project.pdf" target="_blank">View the PDF Explanation</a></li>
                    <li>üì• <a href="mnist_tensorflow_project.ipynb" target="_blank">Download the Jupyter Notebook (.ipynb)</a></li>
                </ul>

                <h3>Key Code Snippet</h3>
                <button id="toggle-code" class="toggle-btn">Show Code ‚¨áÔ∏è</button>
                <div id="code-block" class="code-container">
                    <pre><code>
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=5)

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f"Test accuracy: {test_acc}")
                    </code></pre>
                </div>
            </section>

            <section id="bert-project">
                <h2>BERT Disaster Tweet Classification</h2>

                <h3>Project Description</h3>
                <p>
                    This project applies transfer learning with BERT (using Hugging Face Transformers) to classify disaster-related tweets.
                    The dataset was sourced from Kaggle and includes labeled tweets. The model was fine-tuned using a pretrained BERT base,
                    followed by evaluation on test data. The goal was to experiment with minimal compute on Kaggle GPU.
                </p>

                <h3>Skills Highlighted</h3>
                <ul>
                    <li>Natural Language Processing (NLP)</li>
                    <li>BERT and Transfer Learning</li>
                    <li>Text Preprocessing</li>
                    <li>Model Evaluation</li>
                </ul>

                <h3>Project Files</h3>
                <ul>
                    <li>üìÑ <a href="bert-disaster-tweet_lower_acc.pdf" target="_blank">View the PDF Explanation</a></li>
                    <li>üì• <a href="bert-disaster-tweet_lower_acc.ipynb" target="_blank">Download the Jupyter Notebook (.ipynb)</a></li>
                </ul>

                <h3>Key Code Snippet</h3>
                <button id="toggle-code-bert" class="toggle-btn">Show Code ‚¨áÔ∏è</button>
                <div id="code-block-bert" class="code-container">
                    <pre><code>
from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import AdamW
from tensorflow.keras.optimizers import Adam

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)

optimizer = Adam(learning_rate=2e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])
model.fit(train_dataset, validation_data=val_dataset, epochs=2)

model.evaluate(test_dataset)
                    </code></pre>
                </div>
            </section>

            <section id="more-projects">
                <h2>Other Projects</h2>
                <ul>
                    <li>üß† NLP Tweet Classification using BERT (Kaggle)</li>
                    <li>üìä Time Series Forecasting with LSTM</li>
                    <li>üéØ Titanic Survival Prediction with XGBoost</li>
                    <li>üñºÔ∏è CNN-based Image Classification</li>
                </ul>
                <p>Pages or downloads for these projects coming soon.</p>
            </section>
        </main>

        <footer>
            <p>&copy; <span id="year"></span> Dr. Orsan Kilicer</p>
            <p>
                <a href="https://scholar.google.com/citations?user=UDN1JFMAAAAJ" target="_blank">Google Scholar</a> |
                <a href="https://www.linkedin.com/in/orsan-kilicer" target="_blank">LinkedIn</a>
            </p>
        </footer>
    </div>

    <script>
        // Set current year
        document.getElementById('year').textContent = new Date().getFullYear();

        // MNIST toggle
        const toggleButton = document.getElementById('toggle-code');
        const codeBlock = document.getElementById('code-block');
        toggleButton.addEventListener('click', () => {
            if (codeBlock.style.maxHeight === '0px' || codeBlock.style.maxHeight === '') {
                codeBlock.style.maxHeight = codeBlock.scrollHeight + 'px';
                toggleButton.textContent = 'Hide Code ‚¨ÜÔ∏è';
                toggleButton.style.backgroundColor = '#004494';
            } else {
                codeBlock.style.maxHeight = '0';
                toggleButton.textContent = 'Show Code ‚¨áÔ∏è';
                toggleButton.style.backgroundColor = '#0056b3';
            }
        });

        // BERT toggle
        const toggleBertButton = document.getElementById('toggle-code-bert');
        const bertCodeBlock = document.getElementById('code-block-bert');
        toggleBertButton.addEventListener('click', () => {
            if (bertCodeBlock.style.maxHeight === '0px' || bertCodeBlock.style.maxHeight === '') {
                bertCodeBlock.style.maxHeight = bertCodeBlock.scrollHeight + 'px';
                toggleBertButton.textContent = 'Hide Code ‚¨ÜÔ∏è';
                toggleBertButton.style.backgroundColor = '#004494';
            } else {
                bertCodeBlock.style.maxHeight = '0';
                toggleBertButton.textContent = 'Show Code ‚¨áÔ∏è';
                toggleBertButton.style.backgroundColor = '#0056b3';
            }
        });
    </script>
</body>
</html>
